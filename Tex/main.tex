\documentclass[hidelinks,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

%% A SUPPRIMER A LA FIN
\usepackage[french]{babel}

\usepackage{times}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption,graphicx,enumitem}
\usepackage{amsmath}
%\usepackage{titlesec}

%\setcounter{secnumdepth}{4}


\begin{document}

\title{Study of a surrogate model for shallow water equations}
\author{Bastien Nony, Alban Gossard\\
Institut National des Sciences Appliqu√©es,\\
Toulouse,\\
\href{mailto:nony@etud.insa-toulouse.fr}{   \texttt{nony@etud.insa-toulouse.fr}}\\
\href{mailto:gossard@etud.insa-toulouse.fr}{   \texttt{gossard@etud.insa-toulouse.fr}}}
\date{\today}

\maketitle

\begin{abstract}
In this work we study surrogates problems for different types of modelling problem. The objective is to provide fast calculation for undetermined values. Beginning from physical equations such as Saint-Venant's, we add statistical formulas to determine the variability of the system. This study registers in the frame of geostatistics.
\end{abstract}

\newpage

\tableofcontents

\section{Introduction}

The resources management and the flood forecast requires a solid anticipation which relies onto solid hydraulic models. The numerical breakthroughs allowed huge progress in computational efficiency.

The shallow water equations (SWE) derived from the Navier Stokes for free surface flows proved their efficiency for flood problems in open channels. Regardless, any prediction requires a good knowledge of certain initial values : physical coefficient such as friction and initial flow rate, river bathymetry and boundary conditions. These values vary over time. Because of a lack of knowledge we must assume the input parameters are subject to hazard. Then, variability exists and can come from the nature of the numerical model but also from the environment such as the weather or the season. With this idea we proceed to uncertainty quantification and study output errors and the influence of each input parameter which could limit the effectiveness of the forecast. The objective of uncertainty quantification is to study and to classify the different sources of variability to limit the errors in output. The aim is to better build numerical models to make them less fragile in front of uncertainties, to propose better estimates for reduced computation time.

Indeed, physico-numerical models are long to compute. To avoid this problem we generate a surrogate model given a few number of values. The idea is the following : the original based on physical equations give the best approximation of reality we can expect. We use this model to determine a few amount of points and given these results we interpolate them to have a full model on our study area. 
We consider certain input parameters. As we will see in the following, in the "Chanel Flow" model we consider two input parameters : $Q$ which is the initial flow and $K_s$ the Strickler's coefficient. Because the parameters can vary in time, we model them using random variables. The calibrations are given by experts. 
If the model's computation is too time-consuming, we prefer to evaluate a surrogate model cheaper in time-cost. In our example we will choose two different types of models : kriging and polynomial chaos. Because surrogate models are just an approximation of the physical ones, we have to evaluate uncertainty and the influence of each parameters on the output values.

We will introduce briefly the different tools used in our projects. The first tests will be set on the "Chanel flow" model and the Mickalevitch function. In the last part we will test the efficiency of a surrogate model based on values coming from the Garonne's river, computed using MASCARET-TELEMACS.




\section{Aim of this study and contextualization}

\section{Surrogate Models}

Calculating all values is impossible for computing time reasons. Suppose we wanted to determine the distribution of coal density over each square meter of the Lorraine region, the task would be long and tedious. Each statement would cost time and money. Instead of taking too many measurements on each square meter, we propose to reduce the number of measurements and then interpolate the data over the entire space map. This problem gave rise to the so-called kriging technique.

A first idea would be to interpolate deterministically by linear, polynomial interpolation. This method raises several problems. It will be impossible by mathematical formalism to quantify uncertainty. Indeed, the deterministic variables do not make it possible to estimate potential/unforeseeable variations. For this reason, an alternative proposes a stochastic interpolation.

In this paper we use different methods for computing surrogate models. In this part we present some of their main characteristics.


\subsubsection{Kriging}

Kriging consists in creating a linear estimator from measurements already made before. Using the physical model, $n$ output values $(Y_1,\ldots,Y_n)$ are determined for inputs distributed over our study set. 

The estimator noted $\hat{Y}$ then : $\hat{Y}=\sum_{i=1}^{n}\lambda_i Y_i$ where $\lambda_i$ are scalar values associated to each $Y_i$.

This estimator has several advantages. It is the best linear unbiased predictor. This means it minimizes the variance of errors $Var[\hat{Y}-Y]^2$, it is a linear combination of the measures $Y_i$ and it is unbiased $E[\hat{Y}-Y]=0$.
\begin{enumerate}
\item Also, $\hat{Y}$ admits the perfect values at the $Y_i$ measures.
\item To infinity the points do not bring any more information on the result.
\item The estimator is a convex combinaison of the measures : $\sum_{i=1}^{n}\lambda_i=1$
\item If there are many values in a given region then the weights are low. This is because each point has a greater impact on areas that is close to it and less on remote areas where information is shared between a lot of data points.
\item In areas where there is little data, kriging reflects an estimate of the average. Indeed the values influence in a roughly equivalent way on the very distant points.
\end{enumerate}

\subsubsection{Ordinary kriging solving system}
We place ourselves in the framework of a stationary random function. Thus:

$$E[Y_i]=E[Y_j]=\mu, \forall i,j$$

and $$Cov(X_i,X_{i+h})=\gamma(h), \forall i$$

The fact that our kriging estimator is of minimal variance and is unbiased at points $Y_i$ gives us:

$$E[Y_0-Y_i]=0$$
So, $$\sum_{i=0}^{n}\lambda_iE[Y_i]=E[Y_i]$$
Then, $$\mu \sum_{i=1}^{n}\lambda_i = \mu$$

If we don't know the value of $\mu$, we need to have $$\sum_{i=1}^{n}\lambda_i=1$$

The second condition on the minimal variance gives : 
$$ (\lambda_1,\ldots,\lambda_n)= \arg\min_{\sum \lambda_i=1} Var(\hat{Y}_0-Y_0)$$

We solve this convex optimization problem using the following lagrangian :

$$L(\alpha)=Var(\hat{Y}_0-Y_0)+\alpha(\sum_{i=0}^{n}\lambda_i-1)$$

This brings us to the following result :

$$\forall i \in {1,\ldots, n}, \sum_{i=1}^{n}\lambda_j Cov(Y_i,Y_j)+ \alpha = Cov(Y_0,Y_i)$$

Using matrices we need to solve the following system :

$$
\begin{bmatrix}
K_{11}& \cdots  & K_{1n} & 1\\ \vdots & \ddots & \vdots & \vdots \\ K_{n1} & \cdots  & K_{nn} & 1 \\ 1 & \cdots & 1 & 0 
\end{bmatrix} 
\begin{bmatrix}
\lambda_1 \\ \vdots \\ \lambda_n \\ - \mu 
\end{bmatrix} = 
\begin{bmatrix} K_{10} \\ \vdots \\ K_{n0} \\ 1
\end{bmatrix}.$$

where $K_{ij}=Cov(Y_i,Y_j)$
\\\\
The covariances $K_{ij}, \forall i,j\in {1,\ldots,n}$ are not known.
\\
Covariances $K_{0i}, \in {1,\ldots,n}$ even less. How can we estimate them?

\subsubsection{Concepts of variogram and empirical variograms}

The following function is called variogram : $$\forall i,j, \gamma(h)=Var(\hat{Y}-Y)=E[|\hat{Y}-Y|^2]$$

For simplification, we assumed second order stationarity of the random function Y. So we have :

$$\forall i,j, Cov(Y_i,Y_{j})=C(|i-j|)$$ only depends on the distance between i and j.
\\
Furthermore, $$\gamma(h)\frac{1}{2}Var(\hat{Y}-Y)=\frac{1}{2}(Var(\hat{Y})+Var(Y)-2Cov(\hat{Y},Y))$$
Then,$$\gamma(h)=C(0)-C(h)$$
\\
An estimator of the variogram is the empirical variogram defined as follows :

$$\gamma(h)=\frac{1}{2} \frac{1}{N} \sum_{i=1}^{N} (Y_i - Y_{i+h})^2$$
\\
BATMAN assumes an exponential shape model: 
$$C(h)=\left(1- \exp{\left(-\frac{h}{r}\right)}\right)$$





\section{Study of the 1D model}

Building a surrogate model requires to know if we need surrogate's parameters such as the degree for polynomial chaos expansion. Before setting up a surrogate model based on MASCARET-TELEMACS, one can determine these parameters using shallow water equations.

For both PC expansion and kriging surrogates we determine an efficient initial sample size for surrogate computing. An efficient sample size is a number of sample giving a trade-off between heavy calculations and a quadratic error $Q_2$ not too far away from $1$. For PC expansion surrogate, we need to determine the polynomial degree. This parameter determine the number of polynomial coefficients we have to estimate. If we want the surrogate model to fit the training data, we need the sample size to be large enough considering the degree.

%\subsection{Presentation}
%\subsection{Theoretical tools}
%\subsection{Surrogate model analysis}
%\subsubsection{First example : Ishigami}
%\subsubsection{Garonne Model }

\input{garonne_model}

%\subsubsection{Michalewicz example}







\end{document}
