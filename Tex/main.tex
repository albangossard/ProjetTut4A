\documentclass[hidelinks,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage[french]{babel}
\usepackage{times}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption,graphicx,enumitem}
%\usepackage{titlesec}

%\setcounter{secnumdepth}{4}


\begin{document}

\title{Research project report}
\author{Bastien Nony, Alban Gossard\\
Institut National des Sciences Appliquées,\\
Toulouse,\\
\href{mailto:nony@etud.insa-toulouse.fr}{   \texttt{nony@etud.insa-toulouse.fr}}\\
\href{mailto:gossard@etud.insa-toulouse.fr}{   \texttt{gossard@etud.insa-toulouse.fr}}}
\date{\today}

\maketitle

\begin{abstract}
In this work we study surrogates problems for different types of modelling problem. The objective is to provide fast calculation for undetermined values. Beginning from physical equations such as Saint-Venant's, we add statistical formulas to determine the variability of the system. This study registers in the frame of geostatistics.
\end{abstract}

\newpage

\tableofcontents

\section{Introduction}

The resources management and the flood forecast requires a solid anticipation which relies onto solid hydraulic models. The numerical breakthroughs allowed huge progress in computational efficiency.

The shallow water equations (SWE) derived from the Navier Stokes for free surface flows proved their efficiency for flood problems in open channels. Regardless, any prediction requires a good knowledge of certain initial values : physical coefficient such as friction and initial flow rate, river bathymetry and boundary conditions. These values vary over time. Because of a lack of knowledge we must assume the input parameters are subject to hazard. Then, variability exists and can come from the nature of the numerical model but also from the environment such as the weather or the season. With this idea we proceed to uncertainty quantification and study output errors and the influence of each input parameter which could limit the effectiveness of the forecast. The objective of uncertainty quantification is to study and to classify the different sources of variability to limit the errors in output. The aim is to better build numerical models to make them less fragile in front of uncertainties, to propose better estimates for reduced computation time.

Indeed, physico-numerical models are long to compute. To avoid this problem we generate a surrogate model given a few number of values. The idea is the following : the original based on physical equations give the best approximation of reality we can expect. We use this model to determine a few amount of points and given these results we interpolate them to have a full model on our study area. 
We consider certain input parameters. As we will see in the following, in the "Chanel Flow" model we consider two input parameters : Q which is the initial flow and $K_s$ the Strickler's coefficient. Because the parameters can vary in time, we model them using random variables. The calibrations are given by experts. 
If the model's computation is too time-consuming, we prefer to evaluate a surrogate model cheaper in time-cost. In our example we will choose two different types of models : krigeage and Polynomial chaos. Because surrogate models are just an approximation of the physical ones, we have to evaluate uncertainty and the influence of each parameters on the output values.

We will introduce briefly the different tools used in our projects. The first tests will be set on the "Chanel flow" model and the Mickalevitch function. In the last part we will test the efficiency of a surrogate model based on values coming from the Garonne's river, computed using MASCARET-TELEMACS.


\section{Objectives}
\section{Key notions}


\section{Sources d’incertitude}

ATTENTION : paragraphes à approfondir car une seule source (Fajraoui_noura2014)


\subsection{Incertitude structurelle }

Incertitude liée au modèle mathématique, approximation de la réalité. Les hypothèses simplifient les phénomènes physiques et/ou ne les prennent pas tous en considération. Dans notre cas les équations de Saint Venant sont des équations 1D en eau peu profonde.

\subsection{Incertitude numérique}


Imprécisions liées aux approximations numériques qui s’accumulent dans les calculs. les solutions ne sont pas forcément analytiques et chaque étape de calcul peut conduire à une approximation qui s’additionne aux précédentes, par exemple enchaînement d’erreurs d’arrondi. L’incertitude numérique concerne aussi l’erreur de discrétisation spatiale obtenue lors des relevés physiques effectués par les spécialistes sur le terrain.

\subsection{Incertitude paramétrique}

Incertitude obtenue par la variabilité des paramètres d’entrée ou du manque d’information ou du biais d’un échantillon de départ. Un test statistique effectué sur un échantillon non représentatif peut mener à des résultats complètement biaisés.



\section{Interpolation par krigeage}

Calculer toutes les valeurs est temporellement impossible. Supposons que l'on veuille déterminer la réparitition de la densité de charbon sur chaque mètre carré de la région Lorraine, la tâche serait longue et fasitidieuse. Chaque relevé coûterait du temps et de l'argent et serait surtout inutile. Plutôt que d'effectuer nos mesures sur chaque mètre carré, on se propose d'effectuer des mesures tous les 500 mètres et d'ensuite relier les données sur l'ensemble du territoire. Cette façon de procéder conduit à la notion d'interpolation.

Une première idée serait d'interpoler de manière déterministe par interpolation linéaire, polynomiale, etc. Cette méthode pose plusieurs problèmes. Il sera impossible par formalisme mathématique de quantifier l'incertitude. En effet, les variables déterministes ne permettent pas d'estimer des variations potentielles/non prévisibles. Pour cette raison principalement une alternative propose une interpolation stochastique. Plusieurs méthodes peuvent être proposées : méthode de quadrature, krigeage, polynômes du chaos.

Introduisons quelques outils statistiques :

\subsection{Variogramme}





\subsection{Interpolation par krigeage}

Cette méthode est probablement la plus évidente relativement au formalisme mathématique. Il s'agit d'une estimation linéaire où chaque estimation se fera linéairement en fonction de valeurs exactes mesurées au environs et associées à des poids particuliers.

Ainsi, la procédure est la suivante :

(1) On délimite notre zone d'étude en intervalles de paramétrage.

(2) On mesure, calcule à partir d'un modèle physique de départs un certain nombre de données idéales (X_1,$\ldots$,X_n)

(3) Pour un certain point donné dans notre espace délimité on trouve l'estimateur associé en calculant la matrice de poids correspondante.
(4) notre estimateur en ce point vaut donc la somme des valeurs idéales * leur poids estimé correspondant

Mathématiquement cela donne :

(1) pour un krigeage simple




(2) Pour un krigeage ordinaire

Cette approche apporte plusieurs avantages :


Tout d'abord il est possible de quantifier la variance de notre estimation par le biais de la variance du krigeage :



Les poids de krigeage adoptent des caractéristiques logiques :

(1)A l'infini les points n'apportent plus d'information sur le résultat

(2) Si le nombre de valeurs dans une région donnée est grand alors les poids sont très faibles. Cela s'explique par le fait que chaque point influe davantage sur les zones qui lui sont très proches et moins sur les zones lointaines où l'information se partage entre beaucoup de données.

(3) Dans les régions où il y a peu de données, le krigeage reflète une estimation de la moyenne.

\section{Méthode de quadrature}
\section{Interpolation par polynomes du chaos}


Comment choisir les points de référence? Méthode quasi aléatoires
\section{Méthodes d’échantillonage}

Dans ce paragraphe nous traitons la problématique de l’échantillonnage de l’espace de départ. L’échantillon de départ est limité à N données. Comment les choisir de manière optimale? 

Une première idée consiste à les quadriller de manière régulière notre espace de départ. Pour des raisons de périodicité et de régularité cette méthode peut conduire à l’obtention d’un échantillon non représentatif de la population de départ. Par exemple nous voulons modéliser la fonction sinus sur l’intervalle [0,2k$\pi$]. Si les relevés sont effectués régulièrement tous les 2i$\pi$ alors on pourra penser que la fonction sinus est constante égale à 1. 

D’autres approches permettent de limiter ce risque. Nous allons tout d’abord voir les méthodes de Monte-Carlo puis des méthodes quasi-aléatoires comme la méthode de séquencement de Halton.

\subsection{Méthodes de Monte-Carlo}

Les échantillons ont tendance à se regrouper autour de la région à forte densité


\subsection{Latin Hypercube Sampling}

Meilleur couverture du sampling

\subsection{Séquence de halton}




\section{Study of 1D model}
\subsection{Presentation}
\subsection{Theoretical tools}
\subsection{Surrogate model analysis}
\subsubsection{First example : Ishigami}
\subsubsection{Garonne Model }

\input{garonne_model}

\subsubsection{Michalewicz example}





%\section{Annex}

%\subsection{Polynomial Chaos} ?????




\end{document}
